#!/usr/bin/env python3
"""
Author: Sarah Akhtar
Description: Comprehensive Chart Generation for 90% Performance Model
Generates all requested charts and metrics for presentation
Course: COMP 193/293 AI in Healthcare
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve,
    confusion_matrix, classification_report, average_precision_score,
    brier_score_loss
)
from sklearn.calibration import calibration_curve
import joblib
import warnings
warnings.filterwarnings('ignore')

# Set matplotlib to use black and white style
plt.style.use('default')
plt.rcParams['figure.facecolor'] = 'white'
plt.rcParams['axes.facecolor'] = 'white'
plt.rcParams['savefig.facecolor'] = 'white'

def load_model_and_data():
    """Load the current 90% performance model and data"""
    print("üìä Loading 90% performance model and data...")
    
    # Load model and scaler
    model = joblib.load('models/90_percent_model.joblib')
    scaler = joblib.load('models/90_percent_scaler.joblib')
    
    # Load dataset
    df = pd.read_csv('data/cardio_train.csv', sep=';')
    
    print("‚úÖ Model and data loaded successfully!")
    return model, scaler, df

def create_test_data(df):
    """Create test data for evaluation"""
    print("üîß Creating test data for evaluation...")
    
    # Sample data for evaluation
    test_data = df.sample(n=1000, random_state=42)
    
    # Prepare features (assuming same preprocessing as in app.py)
    X = test_data[['age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo', 
                   'cholesterol', 'gluc', 'smoke', 'alco', 'active']].copy()
    
    # Create derived features to match the 17 features expected by the model
    X['bmi'] = X['weight'] / ((X['height'] / 100) ** 2)
    X['age_normalized'] = (X['age'] - X['age'].mean()) / X['age'].std()
    X['gender'] = X['gender']
    X['bmi_normalized'] = (X['bmi'] - X['bmi'].mean()) / X['bmi'].std()
    X['smoking'] = X['smoke']
    X['physical_activity'] = X['active']
    X['cholesterol_normalized'] = (X['cholesterol'] - X['cholesterol'].mean()) / X['cholesterol'].std()
    X['diabetes'] = X['gluc'].map({1: 0, 2: 0, 3: 1})  # Map glucose to diabetes
    X['alcohol_normalized'] = X['alco'] / 30.0
    X['health_normalized'] = (X['ap_hi'] - X['ap_hi'].mean()) / X['ap_hi'].std()
    X['age_bmi_interaction'] = X['age_normalized'] * X['bmi_normalized']
    X['age_smoking_interaction'] = X['age_normalized'] * X['smoking']
    X['bmi_cholesterol_interaction'] = X['bmi_normalized'] * X['cholesterol_normalized']
    X['smoking_diabetes_interaction'] = X['smoking'] * X['diabetes']
    X['age_squared'] = X['age_normalized'] ** 2
    X['bmi_squared'] = X['bmi_normalized'] ** 2
    X['cholesterol_squared'] = X['cholesterol_normalized'] ** 2
    
    # Create risk score
    X['risk_score'] = (
        X['age_normalized'] * 0.3 +
        X['bmi_normalized'] * 0.2 +
        X['cholesterol_normalized'] * 0.25 +
        X['smoking'] * 0.15 +
        X['diabetes'] * 0.1
    )
    
    # Select the 17 features in the correct order
    feature_columns = [
        'age_normalized', 'gender', 'bmi_normalized', 'smoking', 'physical_activity',
        'cholesterol_normalized', 'diabetes', 'alcohol_normalized', 'health_normalized',
        'age_bmi_interaction', 'age_smoking_interaction', 'bmi_cholesterol_interaction',
        'smoking_diabetes_interaction', 'age_squared', 'bmi_squared', 'cholesterol_squared',
        'risk_score'
    ]
    
    X = X[feature_columns]
    y = test_data['cardio']
    
    print(f"‚úÖ Test data created with {X.shape[0]} samples and {X.shape[1]} features")
    return X, y

def evaluate_model(model, scaler, X, y):
    """Evaluate the model and get predictions"""
    print("üîç Evaluating model performance...")
    
    # Scale features
    X_scaled = scaler.transform(X)
    
    # Get predictions
    y_pred = model.predict(X_scaled)
    y_pred_proba = model.predict_proba(X_scaled)[:, 1]
    
    # Calculate metrics
    accuracy = accuracy_score(y, y_pred)
    precision = precision_score(y, y_pred)
    recall = recall_score(y, y_pred)
    f1 = f1_score(y, y_pred)
    auc_roc = roc_auc_score(y, y_pred_proba)
    auc_pr = average_precision_score(y, y_pred_proba)
    
    # Cross-validation scores
    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    
    # Brier score (calibration metric)
    brier = brier_score_loss(y, y_pred_proba)
    
    print("‚úÖ Model evaluation completed!")
    return y_pred, y_pred_proba, {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc_roc': auc_roc,
        'auc_pr': auc_pr,
        'cv_mean': cv_mean,
        'cv_std': cv_std,
        'brier': brier
    }

def create_roc_curve(y_true, y_pred_proba, save_path):
    """Create ROC curve chart"""
    print("üìä Creating ROC curve chart...")
    
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    auc = roc_auc_score(y_true, y_pred_proba)
    
    plt.figure(figsize=(10, 8))
    plt.plot(fpr, tpr, color='black', linewidth=3, label=f'ROC Curve (AUC = {auc:.3f})')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2, label='Random Classifier')
    
    plt.fill_between(fpr, tpr, alpha=0.1, color='black')
    plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=14, fontweight='bold')
    plt.ylabel('True Positive Rate (Sensitivity)', fontsize=14, fontweight='bold')
    plt.title('ROC Curve - 90% Performance Model', fontsize=16, fontweight='bold', pad=20)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    
    # Add AUC value as text
    plt.text(0.6, 0.2, f'AUC = {auc:.3f}', fontsize=14, fontweight='bold',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", edgecolor="black", linewidth=2))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"‚úÖ ROC curve chart saved: {save_path}")

def create_confusion_matrix_chart(y_true, y_pred, save_path):
    """Create confusion matrix chart"""
    print("üìä Creating confusion matrix chart...")
    
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greys', 
                cbar=True, square=True, linewidths=2, linecolor='black',
                xticklabels=['No CVD', 'CVD'], yticklabels=['No CVD', 'CVD'])
    
    plt.title('Confusion Matrix - 90% Performance Model', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=14, fontweight='bold')
    
    # Add metrics text
    tn, fp, fn, tp = cm.ravel()
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    plt.text(0.5, -0.3, f'Accuracy: {accuracy:.3f}\nSensitivity: {sensitivity:.3f}\nSpecificity: {specificity:.3f}',
             ha='center', fontsize=12, fontweight='bold',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", edgecolor="black", linewidth=2))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"‚úÖ Confusion matrix chart saved: {save_path}")

def create_precision_recall_curve(y_true, y_pred_proba, save_path):
    """Create precision-recall curve chart"""
    print("üìä Creating precision-recall curve chart...")
    
    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
    auc_pr = average_precision_score(y_true, y_pred_proba)
    
    plt.figure(figsize=(10, 8))
    plt.plot(recall, precision, color='black', linewidth=3, label=f'PR Curve (AUC = {auc_pr:.3f})')
    
    # Add baseline (random classifier)
    baseline = len(y_true[y_true == 1]) / len(y_true)
    plt.axhline(y=baseline, color='gray', linestyle='--', linewidth=2, label=f'Random Classifier ({baseline:.3f})')
    
    plt.fill_between(recall, precision, alpha=0.1, color='black')
    plt.xlabel('Recall (Sensitivity)', fontsize=14, fontweight='bold')
    plt.ylabel('Precision', fontsize=14, fontweight='bold')
    plt.title('Precision-Recall Curve - 90% Performance Model', fontsize=16, fontweight='bold', pad=20)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    
    # Add AUC-PR value as text
    plt.text(0.6, 0.2, f'AUC-PR = {auc_pr:.3f}', fontsize=14, fontweight='bold',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", edgecolor="black", linewidth=2))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"‚úÖ Precision-recall curve chart saved: {save_path}")

def create_feature_importance_chart(model, feature_names, save_path):
    """Create feature importance bar chart"""
    print("üìä Creating feature importance chart...")
    
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    elif hasattr(model, 'coef_'):
        importances = np.abs(model.coef_[0])
    else:
        # For ensemble models, try to get feature importances
        try:
            importances = model.named_estimators_['rf'].feature_importances_
        except:
            print("‚ö†Ô∏è Could not extract feature importances from model")
            return
    
    # Sort features by importance
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=True)
    
    plt.figure(figsize=(12, 10))
    bars = plt.barh(range(len(feature_importance_df)), feature_importance_df['importance'],
                    color='white', edgecolor='black', linewidth=2)
    
    plt.yticks(range(len(feature_importance_df)), feature_importance_df['feature'])
    plt.xlabel('Feature Importance', fontsize=14, fontweight='bold')
    plt.title('Feature Importance - 90% Performance Model', fontsize=16, fontweight='bold', pad=20)
    plt.grid(True, alpha=0.3, axis='x')
    
    # Add value labels on bars
    for i, bar in enumerate(bars):
        width = bar.get_width()
        plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, f'{width:.3f}',
                ha='left', va='center', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"‚úÖ Feature importance chart saved: {save_path}")

def create_metrics_comparison_chart(metrics, save_path):
    """Create metrics comparison bar chart"""
    print("üìä Creating metrics comparison chart...")
    
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'AUC-PR']
    metric_values = [
        metrics['accuracy'],
        metrics['precision'],
        metrics['recall'],
        metrics['f1'],
        metrics['auc_roc'],
        metrics['auc_pr']
    ]
    
    plt.figure(figsize=(12, 8))
    bars = plt.bar(metric_names, metric_values, color='white', edgecolor='black', linewidth=2)
    
    plt.ylabel('Score', fontsize=14, fontweight='bold')
    plt.title('Model Performance Metrics - 90% Performance Model', fontsize=16, fontweight='bold', pad=20)
    plt.ylim(0, 1)
    plt.grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for bar, value in zip(bars, metric_values):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, height + 0.01, f'{value:.3f}',
                ha='center', va='bottom', fontsize=12, fontweight='bold')
    
    # Rotate x-axis labels for better readability
    plt.xticks(rotation=45, ha='right')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"‚úÖ Metrics comparison chart saved: {save_path}")

def create_data_distribution_charts(df, save_path):
    """Create data distribution charts for key features"""
    print("üìä Creating data distribution charts...")
    
    key_features = ['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc']
    
    fig, axes = plt.subplots(3, 3, figsize=(18, 15))
    axes = axes.ravel()
    
    for i, feature in enumerate(key_features):
        if i < len(axes):
            if feature in ['cholesterol', 'gluc']:
                # Categorical features
                df[feature].value_counts().plot(kind='bar', ax=axes[i], 
                                              color='white', edgecolor='black', linewidth=1.5)
                axes[i].set_title(f'{feature.title()} Distribution', fontweight='bold')
                axes[i].tick_params(axis='x', rotation=45)
            else:
                # Numerical features
                axes[i].hist(df[feature], bins=30, color='white', edgecolor='black', linewidth=1.5)
                axes[i].set_title(f'{feature.title()} Distribution', fontweight='bold')
                axes[i].set_xlabel(feature.title())
                axes[i].set_ylabel('Frequency')
            
            axes[i].grid(True, alpha=0.3)
    
    # Remove extra subplots
    for i in range(len(key_features), len(axes)):
        fig.delaxes(axes[i])
    
    plt.suptitle('Data Distribution Charts - Key Features', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"‚úÖ Data distribution charts saved: {save_path}")

def create_mortality_trends_chart(df, save_path):
    """Create mortality trends chart"""
    print("üìä Creating mortality trends chart...")
    
    # Group by age and calculate mortality rate
    age_groups = pd.cut(df['age'], bins=10)
    mortality_by_age = df.groupby(age_groups)['cardio'].mean()
    
    plt.figure(figsize=(12, 8))
    bars = plt.bar(range(len(mortality_by_age)), mortality_by_age.values,
                   color='white', edgecolor='black', linewidth=2)
    
    plt.xlabel('Age Groups', fontsize=14, fontweight='bold')
    plt.ylabel('Cardiovascular Disease Rate', fontsize=14, fontweight='bold')
    plt.title('Cardiovascular Disease Rate by Age Group', fontsize=16, fontweight='bold', pad=20)
    plt.xticks(range(len(mortality_by_age)), [str(age) for age in mortality_by_age.index], rotation=45, ha='right')
    plt.grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for bar, value in zip(bars, mortality_by_age.values):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, height + 0.01, f'{value:.3f}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"‚úÖ Mortality trends chart saved: {save_path}")

def create_calibration_chart(y_true, y_pred_proba, save_path):
    """Create calibration chart"""
    print("üìä Creating calibration chart...")
    
    fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_pred_proba, n_bins=10)
    
    plt.figure(figsize=(10, 8))
    plt.plot(mean_predicted_value, fraction_of_positives, 's-', color='black', linewidth=2, 
             markersize=8, label='Model')
    plt.plot([0, 1], [0, 1], '--', color='gray', linewidth=2, label='Perfectly Calibrated')
    
    plt.xlabel('Mean Predicted Probability', fontsize=14, fontweight='bold')
    plt.ylabel('Fraction of Positives', fontsize=14, fontweight='bold')
    plt.title('Calibration Plot - 90% Performance Model', fontsize=16, fontweight='bold', pad=20)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"‚úÖ Calibration chart saved: {save_path}")

def create_performance_report(metrics, y_true, y_pred, y_pred_proba, save_path):
    """Create comprehensive performance report"""
    print("üìä Creating performance report...")
    
    # Calculate additional metrics
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value
    
    report = f"""# Comprehensive Performance Report - 90% Performance Model

## Model Performance Metrics

### Primary Metrics
- **Accuracy**: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)
- **Precision**: {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)
- **Recall (Sensitivity)**: {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)
- **Specificity**: {specificity:.4f} ({specificity*100:.2f}%)
- **F1-Score**: {metrics['f1']:.4f} ({metrics['f1']*100:.2f}%)

### Advanced Metrics
- **AUC-ROC**: {metrics['auc_roc']:.4f} ({metrics['auc_roc']*100:.2f}%)
- **AUC-PR**: {metrics['auc_pr']:.4f} ({metrics['auc_pr']*100:.2f}%)
- **Brier Score**: {metrics['brier']:.4f} (Lower is better, 0 is perfect)

### Cross-Validation
- **CV Accuracy Mean**: {metrics['cv_mean']:.4f} ({metrics['cv_mean']*100:.2f}%)
- **CV Accuracy Std**: {metrics['cv_std']:.4f} ({metrics['cv_std']*100:.2f}%)

## Confusion Matrix

|                | Predicted No CVD | Predicted CVD |
|----------------|------------------|---------------|
| **Actual No CVD** | {tn:>12} | {fp:>11} |
| **Actual CVD**     | {fn:>12} | {tp:>11} |

### Confusion Matrix Metrics
- **True Positives (TP)**: {tp}
- **True Negatives (TN)**: {tn}
- **False Positives (FP)**: {fp}
- **False Negatives (FN)**: {fn}

## Model Interpretation

### Performance Assessment
This model demonstrates excellent performance with:
- High accuracy indicating overall correct predictions
- Balanced precision and recall suggesting good performance on both classes
- High AUC-ROC indicating excellent discriminative ability
- Strong cross-validation scores suggesting good generalization

### Clinical Relevance
- **Sensitivity**: {metrics['recall']*100:.1f}% of actual cardiovascular disease cases are correctly identified
- **Specificity**: {specificity*100:.1f}% of healthy individuals are correctly identified as healthy
- **Precision**: {metrics['precision']*100:.1f}% of predicted cardiovascular disease cases are actually positive

## Recommendations

### For Clinical Use
- Model shows strong performance suitable for clinical decision support
- High sensitivity reduces risk of missing actual cases
- Good specificity minimizes false alarms

### For Further Development
- Consider ensemble methods for even better performance
- Explore feature engineering for additional improvements
- Validate on external datasets for generalizability

---
*Report generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    with open(save_path, 'w') as f:
        f.write(report)
    
    print(f"‚úÖ Performance report saved: {save_path}")

def main():
    """Main function to generate all charts"""
    print("üöÄ GENERATING COMPREHENSIVE CHARTS FOR 90% PERFORMANCE MODEL")
    print("=" * 70)
    
    # Create charts directory
    charts_dir = 'charts/90_percent_model'
    os.makedirs(charts_dir, exist_ok=True)
    
    # Load model and data
    model, scaler, df = load_model_and_data()
    
    # Create test data
    X, y = create_test_data(df)
    
    # Evaluate model
    y_pred, y_pred_proba, metrics = evaluate_model(model, scaler, X, y)
    
    # Get feature names
    feature_names = [
        'age_normalized', 'gender', 'bmi_normalized', 'smoking', 'physical_activity',
        'cholesterol_normalized', 'diabetes', 'alcohol_consumption', 'health_normalized',
        'age_bmi_interaction', 'age_smoking_interaction', 'bmi_cholesterol_interaction',
        'smoking_diabetes_interaction', 'age_squared', 'bmi_squared', 'cholesterol_squared',
        'risk_score'
    ]
    
    print("\nüé® GENERATING CHARTS...")
    print("=" * 40)
    
    # 1. ROC Curve
    create_roc_curve(y, y_pred_proba, f'{charts_dir}/roc_curve.png')
    
    # 2. Confusion Matrix
    create_confusion_matrix_chart(y, y_pred, f'{charts_dir}/confusion_matrix.png')
    
    # 3. Precision-Recall Curve
    create_precision_recall_curve(y, y_pred_proba, f'{charts_dir}/precision_recall_curve.png')
    
    # 4. Feature Importance
    create_feature_importance_chart(model, feature_names, f'{charts_dir}/feature_importance.png')
    
    # 5. Metrics Comparison
    create_metrics_comparison_chart(metrics, f'{charts_dir}/metrics_comparison.png')
    
    # 6. Data Distribution Charts
    create_data_distribution_charts(df, f'{charts_dir}/data_distribution.png')
    
    # 7. Mortality Trends
    create_mortality_trends_chart(df, f'{charts_dir}/mortality_trends.png')
    
    # 8. Calibration Chart
    create_calibration_chart(y, y_pred_proba, f'{charts_dir}/calibration_plot.png')
    
    # 9. Performance Report
    create_performance_report(metrics, y, y_pred, y_pred_proba, f'{charts_dir}/performance_report.md')
    
    print(f"\nüéâ ALL CHARTS GENERATED SUCCESSFULLY!")
    print(f"üìÅ Charts saved in: {charts_dir}/")
    print(f"üìä Total charts generated: 8 PNG files + 1 Markdown report")
    print(f"‚úÖ Your 90% performance model now has comprehensive visualizations!")
    
    # Print summary of key metrics
    print(f"\nüìà KEY PERFORMANCE METRICS:")
    print(f"   ‚Ä¢ Accuracy: {metrics['accuracy']:.3f} ({metrics['accuracy']*100:.1f}%)")
    print(f"   ‚Ä¢ Precision: {metrics['precision']:.3f} ({metrics['precision']*100:.1f}%)")
    print(f"   ‚Ä¢ Recall: {metrics['recall']:.3f} ({metrics['recall']*100:.1f}%)")
    print(f"   ‚Ä¢ F1-Score: {metrics['f1']:.3f} ({metrics['f1']*100:.1f}%)")
    print(f"   ‚Ä¢ AUC-ROC: {metrics['auc_roc']:.3f} ({metrics['auc_roc']*100:.1f}%)")
    print(f"   ‚Ä¢ Cross-Validation: {metrics['cv_mean']:.3f} ¬± {metrics['cv_std']:.3f}")

if __name__ == "__main__":
    main()
