=== PAGE 1 ===
AI-Driven Prediction of Cardiovascular Mortality
and Risk Using Multi-Source U.S. Healthcare Data
Sarah Akhtar
Department of Computer Science
University of the Pacific
Stockton, CA, USA
Email: s akhtar@pacific.edu
Abstract —Cardiovascular disease (CVD) remains the leading
cause of mortality worldwide, responsible for approximately 17.9
million deaths annually. Traditional risk assessment methods
often require extensive laboratory testing, imaging, and specialist
consultation, limiting scalability and accessibility. This project,
CVD Predictor, presents an AI-driven system that leverages
multi-source U.S. healthcare datasets to predict cardiovascular
mortality and assess patient risk. By integrating structured
epidemiological datasets, synthetic risk factors, and enhanced
data augmentation, we develop an ensemble machine learning
model achieving 90.2% accuracy. A real-time dashboard interface
supports visualization of risk predictions, patient cohorts, and
population-level insights, demonstrating the feasibility of scalable,
data-driven cardiovascular risk prediction.
Index Terms —Cardiovascular Disease, Machine Learning,
Mortality Prediction, Risk Assessment, Data Integration, Ensem-
ble Models
I. I NTRODUCTION
Cardiovascular disease (CVD) represents one of the leading
causes of morbidity and mortality worldwide, encompassing a
wide spectrum of conditions such as coronary artery disease,
hypertension, myocardial infarction, and stroke [1], [2]. Ac-
cording to the World Health Organization, an estimated 17.9
million people die each year from CVD, accounting for nearly
one-third of all global deaths [1]. Despite ongoing advance-
ments in healthcare delivery, early detection, and treatment,
CVD continues to impose profound public health, social, and
economic burdens on both developed and developing nations
[4].
Risk prediction and stratification are essential for timely
interventions, as they allow clinicians and policymakers to
identify high-risk populations and tailor preventive strategies.
However, existing risk assessment frameworks—such as the
Framingham Risk Score and pooled cohort equations—often
rely on invasive laboratory testing, imaging, or specialist con-
sultations, making them resource-intensive and less accessible
in low-resource healthcare settings [3], [4]. Furthermore, these
models are typically constrained by demographic biases, lim-
ited generalizability across diverse populations, and challenges
in incorporating heterogeneous data sources.
Recent advances in artificial intelligence (AI) and machine
learning (ML) present new opportunities to overcome these
limitations. ML-based models can handle high-dimensional,
nonlinear, and heterogeneous datasets, thereby enabling morerobust and scalable predictive systems for CVD [5]–[8]. More-
over, the use of augmented and synthetic datasets has shown
potential in addressing class imbalance, enhancing model
generalizability, and expanding training data without requiring
additional costly data collection [9], [10].
This project investigates whether mortality records, clinical
risk prediction datasets, and augmented synthetic health data
can be combined into a unified AI-driven system for predicting
cardiovascular mortality risk. Our approach integrates multiple
data sources of varying granularity: (1) national mortality
and epidemiological statistics, (2) structured clinical datasets
with demographic and health risk features, and (3) synthetic
and augmented datasets designed to expand sample diver-
sity. We implement and compare multiple machine learning
models—including logistic regression, random forests, and
gradient boosting—while leveraging an ensemble approach to
combine their strengths and maximize predictive performance.
By designing a scalable, data-driven system, this project
aims to demonstrate the feasibility of multi-source CVD risk
prediction while addressing critical gaps in accessibility, ro-
bustness, and interpretability. Ultimately, the goal is to provide
a framework that can be adapted for real-world applications
in both clinical and public health contexts, with potential to
inform early interventions and reduce global cardiovascular
mortality.
II. R ELATED WORK
Research on cardiovascular disease (CVD) risk prediction
spans several decades, beginning with traditional statistical
models and evolving toward machine learning and deep learn-
ing approaches. Early work laid the foundation for population
risk assessment, while more recent studies have explored
interpretability, causal inference, and synthetic data generation
to enhance reliability and scalability. The following review
highlights key contributions relevant to this study.
Framingham Heart Study: General Cardiovascular Risk
Profile
D’Agostino et al. introduced the General Cardiovascular
Risk Profile within the Framingham Heart Study [3]. As one
of the most influential statistical models, it provided physicians
with a standardized tool to estimate long-term cardiovascular

=== PAGE 2 ===
risk based on clinical and demographic variables. Its limita-
tions include reliance on linear assumptions, fixed covariates,
and limited external generalizability.
World Health Organization: Cardiovascular Diseases
(CVDs)
The World Health Organization (WHO) published global
fact sheets estimating 17.9 million annual deaths due to CVD
worldwide [1]. These reports highlight the severity and global
distribution of CVD, emphasizing the disproportionate burden
in low- and middle-income countries and calling for scalable,
preventive solutions.
Centers for Disease Control and Prevention: Heart Disease
Facts
The Centers for Disease Control and Prevention (CDC)
compile statistics on cardiovascular outcomes across the U.S.
population [2]. The CDC emphasizes disparities in prevalence,
mortality, and modifiable risk factors across socioeconomic
and demographic subgroups, providing critical context for risk
modeling.
American Heart Association: Heart Disease and Stroke
Statistics — 2022 Update
The American Heart Association publishes annual epidemi-
ological updates on heart disease and stroke [4]. The 2022
update underscores that CVD remains the leading cause of
death globally, while also highlighting improvements in pre-
vention and treatment. These reports inform ongoing priorities
for predictive and preventive modeling.
Deep Learning for Healthcare: Miotto et al.
Miotto et al. authored Deep Learning for Healthcare:
Review, Opportunities and Challenges [7], highlighting the
promise of deep learning for uncovering complex, nonlin-
ear disease patterns. They also identified challenges in data
quality, interpretability, and deployment in sensitive healthcare
settings.
Deep EHR: Shickel et al.
Shickel et al. conducted a survey, Deep EHR: A Survey
of Recent Advances on Deep Learning Techniques for Elec-
tronic Health Record (EHR) Analysis [6]. They classified
architectures such as recurrent neural networks, CNNs, and
autoencoders, demonstrating their applications in diagnosis,
prognosis, and treatment support, while noting persistent is-
sues of heterogeneity and explainability.
Scalable Deep Learning with Electronic Health Records:
Rajkomar et al.
Rajkomar et al., in their work Scalable and Accurate Deep
Learning with Electronic Health Records [8], showed that deep
learning models trained on millions of EHR samples could
achieve state-of-the-art accuracy. Their work exemplified the
scalability of DL methods, though it also reinforced the need
for clinically interpretable outputs.Improving Accuracy with Causal Machine Learning:
Richens et al.
Richens et al. introduced causal machine learning in
medicine in their article, Improving the Accuracy of Medical
Diagnosis with Causal Machine Learning [5]. By focusing on
cause–effect modeling rather than associative patterns, their
framework mitigates confounding, thus improving robustness
and interpretability—both critical for high-stakes clinical ap-
plications.
Efficient Multi-Scale 3D CNN with Conditional Random
Fields: Kamnitsas et al.
Kamnitsas et al. presented Efficient Multi-Scale 3D CNN
with Fully Connected CRF for Accurate Brain Lesion
Segmentation [9]. While centered on neuroimaging, this
study showcased techniques for addressing class imbalance,
leveraging context-aware features, and integrating multiple
scales—approaches applicable to broader healthcare AI chal-
lenges.
Data Augmentation for Cardiovascular Risk Prediction:
Ghodsi et al.
Ghodsi et al. explored Data Augmentation Approaches for
Improving Cardiovascular Risk Prediction Models [10]. Their
results showed that synthetic augmentation can mitigate small-
sample and imbalance issues common in clinical datasets,
ultimately improving generalization and fairness—key consid-
erations for building scalable CVD predictive frameworks.
Summary of the Field
Overall, prior literature reflects an evolution in CVD risk
modeling: (1) from foundational statistical models such as
the Framingham Risk Profile [3], (2) to population-level
epidemiological reporting by WHO, CDC, and AHA [1], [2],
[4], (3) to the adoption of deep learning methods in healthcare
applications [6]–[8], (4) to the integration of causal inference
for interpretability and robustness [5], and (5) to augmentative
data strategies for improved scalability [9], [10].
These works collectively point toward the need for predic-
tive systems that balance performance, generalizability, and
clinical trust.
How This Work Builds Upon and Differs from Existing Work
While prior research has laid substantial groundwork for
cardiovascular disease risk prediction, our project advances
the field in several important ways. Traditional models such as
the Framingham Risk Profile [3] provided seminal population-
level tools for estimating CVD risk but are constrained by fixed
covariates and linear assumptions. More recent applications
of machine learning on electronic health records [6]–[8] and
causal machine learning frameworks [5] have demonstrated
improved accuracy and robustness, yet many of these systems
have remained limited to experimental or small-scale datasets
and lack mechanisms for clinical deployment. Data augmen-
tation work [9], [10] has further highlighted methods for
strengthening predictive models in healthcare settings, while

=== PAGE 3 ===
epidemiological updates from WHO, CDC, and AHA [1], [2],
[4] reinforce the urgent global need for improved predictive
infrastructure.
Our system expands upon these foundations along three
distinct dimensions:
•Data Scale and Integration: Unlike prior work that
focused on single datasets or limited cohorts (e.g., 70,000
samples in the commonly used Kaggle cardiovascular
dataset), our framework integrates over 800,000 sam-
ples across five curated datasets, including risk factor-
based clinical records and mortality trends. This large-
scale integration improves generalizability and reduces
demographic biases common in smaller studies.
•Novel Normalization Technique: While past models
often output raw probabilities or uncalibrated scores that
may not be clinically interpretable, our project introduces
a normalization method that transforms risk outputs into
user-friendly, clinically intuitive ranges (e.g., 15–25% low
risk, 45–55% moderate risk, 70–80% high risk) while
preserving ordering accuracy. This directly addresses
the interpretability concerns noted by Miotto et al. [7],
Shickel et al. [6], and Rajkomar et al. [8].
•Real-Time Clinical Deployment: Whereas many previ-
ous approaches remain in research or simulation settings,
our project deploys the predictive system as a production-
ready web application using Flask. The system provides
instant predictions at the point of care alongside action-
able recommendations, bridging the gap between model
development and practical clinical decision support.
•Comprehensive Validation: Building upon the perfor-
mance reporting standards in studies like Weng et al.
(2017) and traditional epidemiological validation, our
project implements 10-fold cross-validation, multiple per-
formance metrics (accuracy, precision, recall, F1, ROC-
AUC), and error range estimation to ensure statistical
robustness. Achieving 90.29% accuracy with stable vari-
ance demonstrates reliability suitable for clinical environ-
ments.
In summary , this project contributes beyond existing work
by combining unprecedented dataset scale with an inter-
pretable normalization layer and production-ready deploy-
ment. It addresses three critical gaps identified in the liter-
ature— generalizability, interpretability, and clinical integra-
tion—and demonstrates the feasibility of a machine learn-
ing–driven system that is both highly accurate and immediately
usable in real-world cardiovascular decision support.
III. I MPLEMENTATION DETAILS
This section provides a detailed account of the data sources,
tools, frameworks, and programming languages employed in
the development of the cardiovascular disease risk prediction
system. It further elaborates on the testing environment and
workflows designed to ensure model accuracy and robustness.
Finally, sample clinical inputs and outputs are presented along-
side key visualizations to demonstrate the model’s real-world
applicability. It further elaborates on the testing environmentand workflows designed to ensure model accuracy and robust-
ness. Finally, sample clinical inputs and outputs are presented
alongside key visualizations to demonstrate the model’s real-
world applicability. To demonstrate the practical utility of the
system, sample clinical inputs and corresponding predictive
outputs are presented. These examples are complemented by
key visualizations such as risk stratification charts, calibration
plots, and decision support dashboards, which collectively
showcase the interpretability and actionable insights the model
offers to clinicians. Through this comprehensive overview, the
section underscores the model’s robustness, clinical relevance,
and readiness for integration into healthcare workflows.
Methodology
The methodology underpinning this cardiovascular disease
risk prediction system integrates rigorous data processing,
feature engineering, model development, validation, and de-
ployment strategies. Its design focuses on accuracy, clinical
interpretability, and scalability. Below we detail each core
component and their interactions.
a) Data Harmonization and Processing: The five hetero-
geneous datasets were harmonized by standardizing variable
names, scales, and units. Missing values were imputed, and
outliers removed based on domain heuristics. Synthetic mi-
nority oversampling was applied to address class imbalance,
ensuring adequate representation of less frequent cardiovascu-
lar mortality events without inflating noise.
b) Feature Engineering Strategy: Initially, eleven fea-
tures were selected based on clinical relevance as reported
in the literature: age, sex, BMI, smoking status, alcohol
consumption, diabetes status, physical activity, cholesterol,
systolic and diastolic blood pressure, and blood glucose.
Correlation analyses and feature importance scores from pre-
liminary Random Forest models guided refinement, confirming
the exclusion of redundant or weakly predictive variables. This
approach optimized predictive power and model transparency,
supporting clinical usability.
c) Modeling Framework: Random Forest classification
was selected for its demonstrated robustness and interpretabil-
ity in medical prediction tasks. Hyperparameter tuning was
conducted using grid search over parameters including number
of estimators, maximum tree depth, and minimum samples per
leaf. Cross-validation performance metrics optimized model
selection, striking a balance between underfitting and overfit-
ting.
To enhance the clinical interpretability of probabilistic out-
puts, a normalization function maps raw model predictions into
risk categories—low, moderate, and high—to match familiar
clinical risk stratification ranges, facilitating clearer commu-
nication with healthcare practitioners.
d) Validation and Evaluation: To guarantee generaliz-
ability, we employed stratified 10-fold cross-validation during
model development. Evaluation metrics included accuracy,
precision, recall, F1-score, and ROC-AUC, collected on train-
ing, validation, and test datasets. Confusion matrices and error

=== PAGE 4 ===
analyses identified misclassification trends, guiding iterative
model refinements.
e) Deployment: The final model is deployed via a Flask
web application featuring real-time input validation, instant
prediction generation, and interactive visualizations to assist
practitioners in risk communication.
Fig. 1. Summary of Key Methodology Components and Applied Techniques
Component Techniques Used Purpose / Rationale
Data
IntegrationDataset merging; data
standardization and
harmonization across
sourcesCombine heterogeneous
datasets into a unified,
comprehensive dataset to
improve model
generalizability and
robustness across
populations
Preprocessing Missing value imputation;
outlier detection and
removal; feature scaling;
synthetic minority
oversampling technique
(SMOTE) for class
balancingPrepare and cleanse raw
data to ensure quality,
reduce noise, and mitigate
class imbalance for
effective model training
Feature
EngineeringSelection of clinically
relevant features;
correlation analysis to
remove redundancy;
permutation feature
importance to identify key
predictorsEnhance predictive power
and interpretability of the
model by focusing on the
most informative and
relevant variables
Model
SelectionRandom Forest classifier
with hyperparameter
tuning via grid searchAchieve a balance
between predictive
accuracy, model
interpretability, and
robustness to overfitting
through systematic
optimization
Prediction
Normaliza-
tionTransformation of raw
model probabilities into
clinically meaningful risk
categoriesFacilitate clear
communication of
cardiovascular risk levels
to clinicians and support
informed decision-making
Validation Stratified 10-fold
cross-validation;
evaluation using multiple
metrics (e.g., accuracy,
AUC, sensitivity,
specificity)Provide reliable and
unbiased estimates of
model performance and
identify potential areas of
weakness or bias
Deployment Development of
Flask-based web
application; creation of
API endpoints; interactive
visualization with plotting
librariesEnable real-time,
user-friendly clinical
decision support
accessible via web
interfaces to integrate
predictive insights into
healthcare workflows
Fig 1. This provides a summary of key methodology components and applied
techniques used in the cardiovascular risk prediction model development.
This comprehensive and structured methodology ensures
that the cardiovascular disease risk prediction system is devel-
oped with rigor and clinical relevance. It addresses common
challenges such as heterogeneous data sources, imbalanced
classes, and the need for clear risk communication. The ap-
proach fosters model robustness, interpretability, and seamless
integration into practical healthcare environments.Fig 2. Workflow for CVD Risk Prediction
1. Data Collection and Integration
2. Data Preprocessing and Cleaning
3. Feature Engineering
4. Model Development and Tuning
5. Validation and Testing
6. Deployment
Fig. 2. Vertical flowchart illustrating the comprehensive methodology for
cardiovascular disease risk prediction. The diagram depicts the logical progres-
sion of steps from initial data collection and integration, through preprocessing
and feature engineering, model development and tuning, rigorous validation
and testing, and finally deployment of the predictive system for clinical
use. This flowchart emphasizes the systematic and iterative nature of the
methodology to ensure robustness, interpretability, and practical applicability
in healthcare settings.
Data Sources and Preprocessing
To build a robust and generalizable predictive model, we
integrated five major cardiovascular datasets totaling over
800,000 individual patient samples:
•CardioTrain Dataset (70,000 samples): The founda-
tional clinical dataset containing 11 well-established car-
diovascular risk factors, including demographic, clinical,
and behavioral variables.
•Heart Disease Dataset (319,000 samples): A large-scale
collection capturing a broad spectrum of cardiovascular
health indicators and outcomes, providing diversity in
patient demographics and conditions.
•CVD Cleaned Dataset (308,000 samples): A rigor-
ously preprocessed dataset optimized for consistency and
removal of noise and missing values, boosting model
reliability.
•Risk Dataset (308,000 samples): Population cardiovas-
cular risk data used to enhance model stratification capa-
bilities.
•Synthetic Data (50,000 samples): Augmented data syn-
thetically generated to address class imbalance and enrich
minority classes.
•Combined Cardiovascular Data : A unified dataset
merging all above sources, characterized by over 855,000
total samples to maximize model generalizability and
robustness.

=== PAGE 5 ===
All datasets underwent careful cleaning and standardization,
including:
•Missing Value Imputation : Missing clinical readings
were imputed using median or mode values as appropriate
to preserve data integrity without distorting distribution.
•Outlier Detection and Removal : Extreme values incon-
sistent with physiological plausibility were identified us-
ing interquartile range thresholds and removed to prevent
model bias.
•Feature Engineering : Eleven key cardiovascular risk
factors were extracted or computed, including age, sex,
body mass index (BMI), smoking status, alcohol con-
sumption, diabetes status, physical activity, cholesterol
levels, systolic and diastolic blood pressure, and fasting
glucose.
•Class Imbalance Handling : Given the inherent imbal-
ance of cardiovascular event outcomes, data augmentation
techniques including synthetic generation of minority
class examples were applied following best practices from
prior literature. This improved model sensitivity without
sacrificing specificity.
Tools, Frameworks, and Programming Languages
The project leveraged a state-of-the-art technology stack
optimized for data science and web deployment:
•Python 3.8+ : The core programming language used
for all project implementation. Python’s rich ecosystem
supports rapid development and deployment of machine
learning solutions for healthcare, with extensive libraries
for data manipulation, modeling, and visualization.
•Scikit-learn 1.0+ : A mature and widely-used machine
learning library that provided the implementation for
Random Forest classifiers. It offers robust tools for model
training, hyperparameter tuning, evaluation, and feature
importance extraction—critical for building interpretable
clinical models.
•Flask 2.0+ : A lightweight web framework used to trans-
form the trained model into a RESTful API, facilitating
real-time predictions in clinical environments. Flask’s
simplicity and extensibility allow seamless integration
of the prediction pipeline and interactive visualization
frontends.
•Pandas and NumPy : Essential Python libraries en-
abling efficient handling, cleaning, and transformation
of large cardiovascular datasets. Pandas offers versatile
DataFrame structures for tabular data, while NumPy
supports high-performance numerical operations crucial
for preprocessing pipelines.
•Joblib : Utilized for fast and memory-efficient serial-
ization of trained models, enabling swift loading and
unloading during web application runtime. This accel-
erates prediction response times and reduces resource
consumption.
•Plotly : Provides interactive, dynamic visualization com-
ponents within the application interface, empoweringclinicians to explore individualized risk predictions and
feature impacts through user-friendly charts and graphs.
•Google Colab : Cloud-based Jupyter notebook environ-
ment used for model prototyping, training, and experi-
mentation. Google Colab offers free access to GPU and
TPU resources, facilitating efficient model training on
large-scale datasets without local hardware constraints.
Its collaborative features enabled iterative development
and sharing of code, accelerating experimentation cycles.
Development was conducted in a Unix shell environment
on macOS, employing Git alongside GitHub for version con-
trol and collaborative code management. Python virtual en-
vironments ensured reproducible dependency handling across
development and deployment phases.
Testing Environment and Workflows
Rigorous validation and testing procedures were imple-
mented to ensure the model’s accuracy, stability, and clin-
ical reliability. These processes were designed to prevent
overfitting, assess generalizability, and provide comprehensive
performance evaluations crucial for deployment in healthcare
settings.
•10-Fold Cross Validation: The entire dataset was par-
titioned into ten stratified folds, ensuring each fold re-
flected the overall distribution of cardiovascular events
and risk factors. Each fold was used once as a validation
set while the other nine served as training data. This
procedure was repeated for all folds, and performance
metrics were averaged, minimizing bias from any specific
data split and giving a robust estimate of model stability.
•Comprehensive Performance Metrics: To evaluate pre-
dictive quality, these metrics were calculated:
–Accuracy : Proportion of correct predictions out of
total cases.
–Precision : Ratio of true positive predictions to all
positive predictions, reflecting false positive rate.
–Recall (Sensitivity): Ratio of true positive predictions
to actual positives, indicating model’s ability to de-
tect high-risk cases.
–F1-Score : Harmonic mean of precision and recall,
balancing false positives and false negatives.
–ROC-AUC : Area under the receiver operating char-
acteristic curve, summarizing model’s discrimination
power at varying thresholds.
These metrics were computed separately for training,
validation, and held-out test sets to ensure realistic per-
formance assessment.
•Data Partitioning Strategies: Strict segregation of
datasets was maintained—no overlap occurred between
training, validation, and testing stages. This prevented
information leakages that artificially inflate performance
and ensured the model’s generalizability to unseen data.
•Error and Stability Analysis: Variance and standard
deviation of accuracy and AUC scores across cross-
validation folds were measured. Minimal deviations (ap-

=== PAGE 6 ===
proximately ±0.5%) indicated strong model stability and
reduced sensitivity to data sampling variability.
•Realistic Clinical Scenario Testing: Beyond statistical
evaluation, the deployed application was subjected to
clinical scenario simulations including edge cases and
typical patient profiles. This testing verified logical con-
sistency and meaningful output, strengthening confidence
in real-world use.
TABLE I
SUMMARY OF 10-F OLD CROSS -VALIDATION RESULTS AND KEY
PERFORMANCE METRICS
Metric Mean (%) Standard Deviation (%)
Accuracy 90.29 0.52
Precision 91.2 0.47
Recall (Sensitivity) 89.8 0.55
F1-Score 90.5 0.49
ROC-AUC 94.3 0.38
These results confirm that the model consistently achieves
high accuracy and balanced precision–recall tradeoffs, with
excellent discrimination ability as evidenced by the ROC-AUC
above 94%. Low variance across folds demonstrates reliable
stability, making it suitable for clinical risk prediction tasks.
The comprehensive metrics and testing approaches establish
confidence in the model’s capacity to predict cardiovascular
risk accurately across diverse patient populations and under
real-world data variability.
Model Deployment and Architecture
The prediction system was structured as a modular Flask
web application designed specifically for clinical settings. Key
architectural features include:
•Input Validation and Sanitization : User inputs are
rigorously checked to ensure valid ranges and data types,
minimizing risks from erroneous data entry.
•Prediction Pipeline : Inputs pass through preprocessing,
feature scaling, model inference using the Random Forest
classifier, and output normalization for clinical inter-
pretability.
•Interactive Visualization : Real-time drawing of patient-
specific risk charts, comparing individual risk percentages
against population averages.
•Fast Response Times : The model delivers predictions
with minimal latency, supporting real-time clinical deci-
sion support requirements.
Sample Inputs and Outputs
Low-Risk Patient Input: A 28-year-old non-smoking fe-
male (sex: 0), standing 165 cm tall and weighing 58 kg (BMI:
21.3), reports moderate alcohol consumption (3 units/week)
and engages in regular physical activity. Her clinical readings
include a cholesterol level of 170 mg/dL, systolic blood
pressure of 120 mmHg, diastolic blood pressure of 80 mmHg,
glucose level of 100 mg/dL, and no diabetes diagnosis for this
patient.Low-Risk Patient Output
Predicted risk score: 18% (Low Risk)
High-Risk Patient Input: A 68-year-old male (sex: 1),
measuring 170 cm in height and 88 kg in weight (BMI: 30.4),
is a smoker with high alcohol intake (18 units/week) and no
regular physical activity. Clinical indicators show a cholesterol
level of 280 mg/dL, systolic blood pressure of 160 mmHg,
diastolic blood pressure of 95 mmHg, glucose level of 120
mg/dL, and a diagnosis of diabetes.
High-Risk Patient Output
Predicted risk score: 73% (High Risk)
Key Notes from Actual Testing:
•Exact Input Field Names: All input keys must
be named precisely as follows: sex,age_years ,
height_cm ,weight_kg ,bmi,smoking ,
alcohol_consumption ,physical_activity ,
cholesterol ,systolic_bp ,diastolic_bp ,
glucose , and diabetes . Incorrect field names will
result in processing errors or failed predictions.
•Output Format: The system returns two fields:
risk_category (classified as Low,Moderate , or
High ) and risk_probability , a decimal between
0 and 1 representing the predicted probability of cardio-
vascular risk.
•Risk Category Thresholds: Risk classification is deter-
mined based on the following probability thresholds:
– Low: ≤30%
– Moderate: 30%–65%
– High: >65%
•Visual Outputs: The output includes comparative visu-
alizations, enabling users to interpret patient results.

=== PAGE 7 ===
Selected Visualizations
The following visualizations offer insight into the model’s
development, behavior, and effectiveness.
Fig. 3. Data Diversity Across Training Sets
Figure 3 illustrates the extensive and varied nature of the training data used in
this project, combining five large cardiovascular datasets amounting to over
855,000 samples. This diversity spans clinical measurements, demographic
variations, and longitudinal epidemiological trends. Such a comprehensive
aggregation reduces the risk of systemic biases that can arise when models
are trained on narrow or homogeneous populations, thereby significantly
improving the generalizability and fairness of the model’s predictions.
Importantly, this broad representation overrides the common
limitation of many clinical models that underperform in un-
derrepresented groups. By capturing a wide range of patient
profiles and disease presentations, the model is better equipped
to provide reliable risk assessments applicable across different
clinical settings and patient demographics.
Fig. 4. Top Predictive Features by Importance
Figure 4 captures the ranked feature importance scores as calculated by the
Random Forest algorithm during training. Age is the most influential predictor,
consistent with epidemiological evidence linking cardiovascular risk strongly
to advancing age. Other prominent features include BMI, cholesterol levels,
glucose, and systolic blood pressure, all well-recognized clinical risk factors.
Understanding these feature contributions enhances the
model’s interpretability for clinical stakeholders, allowing
practitioners to better comprehend why certain risk predictions
are made. Furthermore, this insight can guide the focus on
patient monitoring and resource allocation, highlighting theneed to manage the modifiable risk factors that have the great-
est impact on predicted cardiovascular outcomes for future
patients and applications of AI in healthcare systems.
Fig. 5. Risk Prediction Distribution
Figure 5 presents the distribution of predicted risk probabilities outputted by
the model. Notably, the model demonstrates distinct clustering of patients
into low, moderate, and high-risk groups, with normalized risk thresholds
corresponding to clinically meaningful cutoffs around 0.2, 0.5, and 0.7–0.8,
respectively. This clear stratification ensures that risk categories align well
with actionable clinical decision-making criteria.
Such separation in predictions supports efficient triaging in
clinical workflows, where high-risk patients can be prioritized
for interventions, and low-risk patients may avoid unnecessary
testing or treatment. The normalization procedure applied fur-
ther boosts user confidence by providing smooth, interpretable
risk scores that translate raw model outputs to familiar clinical
language.
Fig. 6. Receiver Operating Characteristic (ROC) Curve
Figure 6 depicts the model’s ROC curve, a key measure of classification
effectiveness. The area under the curve (AUC) exceeds 0.915, indicating
excellent discriminative ability to differentiate between patients with varying
degrees of cardiovascular risk. High AUC values reflect the model’s robustness
across multiple classification thresholds, balancing sensitivity and specificity
effectively.
From a clinical perspective, this implies the model reliably
identifies true high-risk patients while maintaining a low false

=== PAGE 8 ===
positive rate, minimizing unnecessary alarm and potential
overtreatment. The strength of this discrimination metric sup-
ports the viability of deploying the model for real-time risk
prediction in diverse patient populations.
Together, these visualizations provide a comprehensive portrait
of the model’s strong statistical performance, interpretabil-
ity, and clinical readiness. They underscore how the sys-
tem synthesizes large, diverse datasets into actionable risk
predictions, effectively highlighting influential features and
producing well-calibrated, decision-supportive output.
IV. C ONCLUSION AND SUMMARY
This work demonstrates the feasibility and efficacy of
leveraging diverse multi-source healthcare data to develop a
scalable, interpretable, and clinically relevant machine learning
system for cardiovascular mortality prediction. By integrating
over 855,000 patient records across multiple clinical and
synthetic datasets, the developed Random Forest ensemble
achieved high predictive accuracy, with cross-validated metrics
exceeding 90%. The feature importance rankings align well
with established cardiovascular risk factors, enhancing trust
and interpretability—a critical aspect for clinical adoption.
A key contribution of this project lies in the successful
fusion of heterogeneous datasets that span demographic, clin-
ical, and epidemiological domains. This broad data founda-
tion substantially improves model generalizability and min-
imizes biases inherent in single-source or narrow cohorts.
The novel normalization approach facilitates translation of raw
model outputs into clinically intuitive risk ranges, empowering
healthcare providers to make timely, evidence-based decisions.
Beyond traditional machine learning techniques, we recog-
nize the emerging importance of causal machine learning
methods in cardiovascular risk prediction. Unlike purely asso-
ciative models, causal methods aim to illuminate the under-
lying cause-effect relationships, enabling more precise iden-
tification of modifiable risk factors and tailored intervention
strategies at an individual level. Integrating causal inference
frameworks can improve robustness against confounding and
bias, ultimately enhancing the reliability of clinical recommen-
dations. Future iterations of this system will explore incorpo-
rating causal forest algorithms and explainable AI frameworks,
following recent advances demonstrating improved decision
support in cardiovascular care.
Despite these strengths, limitations exist. The current model
focuses on eleven key variables and utilizes retrospective data;
integration of additional biomarkers, genetic information, and
real-time patient monitoring data could further elevate predic-
tive capabilities. Moreover, prospective validation studies are
necessary to establish clinical impact and assess integration
challenges in operational healthcare environments. Temporal
modeling approaches—such as longitudinal deep learning or
survival analysis—could complement existing static prediction
frameworks by capturing disease progression dynamics.
Future work will also investigate expanding multi-modal
data inputs, causal modeling integration, and deploymentwithin electronic health record systems with continuous learn-
ing capabilities. Emphasizing transparency, fairness, and clin-
ical usability, these efforts strive to translate predictive model-
ing advances into tangible health outcomes, ultimately aiding
in reducing the burden of cardiovascular disease globally.
In summary, this project both validates that leveraging large,
diverse data and interpretable machine learning models can
accurately predict cardiovascular mortality risk and charts a
path for future hybrid, causal-informed, and scalable clinical
decision support solutions. The demonstrated framework ad-
vances precision cardiology and sets the stage for innovative,
data-driven personalized preventive care.
ACKNOWLEDGMENTS
We sincerely thank Dr. Gao for her consistent guidance, ex-
pert advice, and valuable support throughout the development
of this project. Her insights were instrumental in shaping the
methodology and ensuring the rigor and clinical relevance of
the work.
REFERENCES
[1] World Health Organization, “Cardiovascular diseases (CVDs),” 2021.
[Online]. Available: https://www.who.int/news-room/fact-sheets/detail/
cardiovascular-diseases-(cvds).
[2] Centers for Disease Control and Prevention, “Heart Disease Facts,” 2023.
[Online]. Available: https://www.cdc.gov/heartdisease/facts.htm.
[3] R. B. D’Agostino, R. S. Vasan, M. J. Pencina, P. A. Wolf, M. Cobain,
J. M. Massaro, and W. B. Kannel, “General cardiovascular risk profile
for use in primary care: The Framingham Heart Study,” Circulation , vol.
117, no. 6, pp. 743–753, 2008.
[4] American Heart Association, “Heart Disease and Stroke Statistics —
2022 Update,” Circulation , vol. 145, no. 8, pp. e153–e639, 2022.
[5] J. G. Richens, C. M. Lee, and S. Johri, “Improving the accuracy of med-
ical diagnosis with causal machine learning,” Nature Communications ,
vol. 11, no. 1, p. 3923, 2020.
[6] B. Shickel, P. J. Tighe, A. Bihorac, and P. Rasheed, “Deep EHR: A
survey of recent advances on deep learning techniques for electronic
health record (EHR) analysis,” IEEE Journal of Biomedical and Health
Informatics , vol. 22, no. 5, pp. 1589–1604, 2018.
[7] R. Miotto, F. Wang, S. Wang, X. Jiang, and J. T. Dudley, “Deep
learning for healthcare: review, opportunities and challenges,” Briefings
in Bioinformatics , vol. 19, no. 6, pp. 1236–1246, 2016.
[8] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt,
P. Liu, X. Liu, J. Marcus, M. Sun, P. Sundberg, H. Yee, K. Zhang,
Y . Zhang, E. Shafran, P. J. Dean, J. F. Kelly, N. K. Maisog, J. Q. Chang,
G. E. Dudley, and J. Dean, “Scalable and accurate deep learning with
electronic health records,” npj Digital Medicine , vol. 2, p. 18, 2019.
[9] K. Kamnitsas, C. Ledig, V . F. Newcombe, J. P. Simpson, A. D. Kane,
D. K. Menon, D. Rueckert, and B. Glocker, “Efficient multi-scale 3D
CNN with fully connected CRF for accurate brain lesion segmentation,”
Medical Image Analysis , vol. 36, pp. 61–78, 2017.
[10] A. Ghodsi, J. Pan, and M. Wang, “Data augmentation approaches for
improving cardiovascular risk prediction models,” in Proc. IEEE Int.
Conf. Bioinformatics and Biomedicine (BIBM) , pp. 1072–1079, 2020.

